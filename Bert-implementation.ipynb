{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c58c1534-5592-40a9-aa10-63ee379643c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "09aced00-a33c-4c53-97ae-4c1d40ea64be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb05594d-e3cb-40f8-851c-1dabb2200af6",
   "metadata": {},
   "source": [
    "### 1. Import relevant libraries and set a random seed for reproducibility of bert model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f0f233f5-e84a-4fba-8a52-48fcd4f3c6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Based on https://www.geeksforgeeks.org/how-to-generate-word-embedding-using-bert/\n",
    "# importing libraries\n",
    "import random\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "612caf6f-07a8-411c-9062-79640826b9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a random seed\n",
    "random_seed = 42\n",
    "random.seed(random_seed)\n",
    "\n",
    "# Set a random seed for PyTorch (for GPU as well)\n",
    "torch.manual_seed(random_seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28c93a1-462c-4aec-85e6-0c6aa729474b",
   "metadata": {},
   "source": [
    "### 2. Load bert tokenizer and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e1bd4847-f936-454b-a892-7f91d0e4fada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BERT tokenizer and model 'bert-base-uncased' is the most common in NLP tasks\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a6012c-569e-4a88-aaeb-71754e34cf1f",
   "metadata": {},
   "source": [
    "### 3. Tokenize and encode sentence, output is token id and token attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "726ce5cf-a784-49df-b389-f04a7b175d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input ID: tensor([[  101, 29294, 22747, 21759,  4402,  5705,  2003,  1037,  3274,  2671,\n",
      "          9445,   999,   102],\n",
      "        [  101,  2023,  2003,  2178,  6251,   102,     0,     0,     0,     0,\n",
      "             0,     0,     0]])\n",
      "Attention mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "# Input text\n",
    "\n",
    "text = [\"GeeksforGeeks is a computer science portal!\" , \"This is another sentence\"]    \n",
    "\n",
    "# Tokenize and encode text using batch_encode_plus\n",
    "# The function returns a dictionary containing the token IDs and attention masks\n",
    "encoding = tokenizer.batch_encode_plus( text,# List of input texts\n",
    "    padding=True,              # Pad to the maximum sequence length\n",
    "    truncation=True,           # Truncate to the maximum sequence length if necessary\n",
    "    return_tensors='pt',      # Return PyTorch tensors\n",
    "    add_special_tokens=True    # Add special tokens CLS and SEP\n",
    ")\n",
    "\n",
    "input_ids = encoding['input_ids']  # Token IDs\n",
    "# print input IDs\n",
    "print(\"Input ID:\", input_ids)\n",
    "attention_mask = encoding['attention_mask']  # Attention mask\n",
    "# print attention mask\n",
    "print(\"Attention mask:\", attention_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb690254-cc06-416c-b4f2-ca6c36e5ce04",
   "metadata": {},
   "source": [
    "### 4. Generate word embeddings using bert model, inputs are token ids and token attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b8ff7881-2b10-4150-baaa-240682e0e3e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Word Embeddings: torch.Size([2, 13, 768])\n"
     ]
    }
   ],
   "source": [
    "# Generate embeddings using BERT model\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    word_embeddings = outputs.last_hidden_state  # This contains the embeddings\n",
    "\n",
    "# Output the shape of word embeddings\n",
    "print(\"Shape of Word Embeddings:\", word_embeddings.shape)\n",
    "\n",
    "# each token has 768 sized vector embedding associated with it, 13 is the number of tokens and 1 refers to the sentence "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fccc366-c738-442b-8c56-ed7673791273",
   "metadata": {},
   "source": [
    "### 5. You can visualize the tokens, corresponding embeddings and decoded text for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cc53c160-ff68-49c7-92a7-9ddf112721bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded Text: geeksforgeeks is a computer science portal!\n",
      "tokenized Text: ['geek', '##sf', '##org', '##ee', '##ks', 'is', 'a', 'computer', 'science', 'portal', '!']\n",
      "Encoded Text: tensor([[101, 100, 100, 102]])\n"
     ]
    }
   ],
   "source": [
    "# Decode the token IDs back to text\n",
    "decoded_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "#print decoded text\n",
    "print(\"Decoded Text:\", decoded_text)\n",
    "\n",
    "# Tokenize the text again for reference\n",
    "tokenized_text = tokenizer.tokenize(decoded_text)\n",
    "#print tokenized text\n",
    "print(\"tokenized Text:\", tokenized_text)\n",
    "\n",
    "# Encode the text\n",
    "encoded_text = tokenizer.encode(text, return_tensors='pt')  # Returns a tensor\n",
    "# Print encoded text\n",
    "print(\"Encoded Text:\", encoded_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0d98f630-67d8-4339-8986-4469cc5bca08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: geek\n",
      "Token: ##sf\n",
      "Token: ##org\n",
      "Token: ##ee\n",
      "Token: ##ks\n",
      "Token: is\n",
      "Token: a\n",
      "Token: computer\n",
      "Token: science\n",
      "Token: portal\n",
      "Token: !\n"
     ]
    }
   ],
   "source": [
    "# Print word embeddings for each token\n",
    "for token, embedding in zip(tokenized_text, word_embeddings[0]):\n",
    "    print(\"Token:\", token)\n",
    "   # print(\"Embedding:\", embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad6ba56-3036-45cb-bc60-72cb98e264c7",
   "metadata": {},
   "source": [
    "### 6. Sentence level embedding for various NLP tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c929a486-8165-4b13-9441-2bd01e7a6198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Embedding\n",
      "Shape of Sentence Embedding: torch.Size([2, 768])\n"
     ]
    }
   ],
   "source": [
    "# Compute the average of word embeddings to get the sentence embedding\n",
    "sentence_embedding = word_embeddings.mean(dim=1)  # Average pooling along the sequence length dimension\n",
    "\n",
    "# Print the sentence embedding\n",
    "print(\"Sentence Embedding\")\n",
    "#print(sentence_embedding)\n",
    "\n",
    "# Output the shape of the sentence embedding\n",
    "print(\"Shape of Sentence Embedding:\", sentence_embedding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81afc443-ec85-491d-b913-afb4566b431e",
   "metadata": {},
   "source": [
    "### 7. NLP task : Check the cosine similarity with another sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "409184b9-ec8b-4508-851d-41e9a77605f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.5059e-02,  2.2726e-01,  3.1609e-01,  1.8410e-01,  1.8076e-01,\n",
       "        -7.8699e-01,  4.3954e-02,  5.0712e-01, -3.1514e-01, -1.4152e-01,\n",
       "         1.5725e-01, -6.4614e-01, -4.3374e-02,  5.9408e-01, -3.6698e-01,\n",
       "        -7.7098e-03, -2.3622e-01,  1.8715e-01, -2.4054e-01, -2.5308e-01,\n",
       "        -1.3704e-01, -2.8994e-01, -7.4470e-02,  4.0289e-01,  2.3488e-01,\n",
       "         5.8243e-03,  1.5201e-01, -4.3085e-02, -1.1464e-01,  7.0477e-02,\n",
       "         2.7549e-01,  4.6293e-01,  8.8530e-02,  9.7691e-02, -5.5874e-01,\n",
       "        -1.9001e-01, -3.6973e-01,  1.1568e-01,  1.1165e-02,  6.9461e-01,\n",
       "         1.7377e-02, -3.4202e-01,  1.2402e-01,  2.3606e-01, -2.3257e-01,\n",
       "        -2.6446e-01, -1.1868e-01,  1.1279e-01, -4.4310e-02, -4.6620e-02,\n",
       "        -4.8528e-01,  4.1347e-01, -2.0555e-01, -5.7425e-02,  2.5642e-01,\n",
       "         6.4752e-01,  8.9251e-02, -3.9436e-01, -2.4395e-01, -2.1154e-02,\n",
       "         1.6391e-01,  2.9419e-01, -7.6012e-02, -2.8513e-01, -3.5262e-03,\n",
       "         3.7497e-02,  7.5735e-02,  5.4062e-01, -7.4250e-01,  2.3180e-02,\n",
       "         1.9096e-03, -5.8958e-03, -1.8274e-01, -5.9895e-02,  1.0690e-01,\n",
       "        -1.5556e-01,  1.1614e-01,  4.3746e-01,  3.7478e-01, -1.6598e-01,\n",
       "        -8.1504e-02,  5.3837e-01,  3.6731e-01,  3.6756e-01,  2.8357e-01,\n",
       "         4.5641e-02, -3.0775e-01,  9.2860e-02, -6.5061e-02,  4.1864e-01,\n",
       "        -1.1066e-01,  6.5182e-02,  4.3878e-01,  2.5640e-01,  7.1872e-01,\n",
       "        -2.8500e-02, -4.2918e-02, -3.3367e-01, -3.0994e-01,  3.8152e-01,\n",
       "         1.8926e-01, -4.8699e-01, -1.7817e-01,  2.5533e-01, -3.6976e-01,\n",
       "         1.4925e-01,  1.6289e-01,  1.5364e-02,  1.3451e-01,  1.5436e-01,\n",
       "         4.4995e-01,  2.5381e-01,  3.2895e-02, -3.5347e-01, -5.6195e-02,\n",
       "         2.3975e-01,  1.0457e-01, -9.6743e-02,  4.0963e-01, -2.0841e-01,\n",
       "         4.6713e-02,  3.3508e-01,  1.4533e-02,  1.0118e+00, -7.1069e-02,\n",
       "         3.3019e-01, -1.2276e-01,  1.3015e-01,  8.8449e-02,  1.8881e-01,\n",
       "         2.3417e-01,  5.1114e-01,  5.0298e-01, -2.8355e-01, -1.0531e-01,\n",
       "         1.4100e-01, -8.7667e-02, -4.2800e-02, -3.9036e-01, -5.1876e-03,\n",
       "        -2.7307e-01, -9.9376e-02, -2.3187e-01, -7.3257e-02,  1.1850e-01,\n",
       "         3.6541e-03, -1.5314e-01,  1.1413e-01, -3.9769e-02, -2.8946e-02,\n",
       "        -2.6575e-01, -2.7251e-02, -3.4345e-01, -5.5521e-01,  2.3927e-01,\n",
       "        -2.2237e-01,  1.7035e-01,  4.3162e-02,  7.4619e-01,  3.6223e-01,\n",
       "         2.3987e-01,  1.3347e-01, -3.5340e-01, -2.1887e-01, -1.7333e-01,\n",
       "        -2.7984e-01,  1.4185e-01,  4.1934e-01,  2.0612e-01,  1.9071e-01,\n",
       "        -2.1247e-01, -4.3117e-01,  4.0103e-01,  3.6068e-01,  3.0649e-01,\n",
       "         4.2015e-02,  5.6213e-01, -2.3547e-01,  4.0205e-01,  5.0571e-01,\n",
       "        -8.1788e-01, -1.8288e-02,  1.2835e-01,  3.0938e-02,  2.0433e-01,\n",
       "         1.1660e-02,  4.1289e-01, -3.9610e-01, -4.2373e-02, -1.0219e-01,\n",
       "        -6.8291e-01, -1.9912e-01, -1.7964e-01,  5.3788e-02,  7.6118e-01,\n",
       "        -4.9347e-01, -2.9235e-01,  2.3222e-01,  1.1968e-01, -3.3852e-03,\n",
       "         3.7796e-01, -5.8655e-02, -1.3723e-01, -7.4061e-02, -4.4270e-01,\n",
       "         3.7834e-01,  1.1861e-01, -3.8189e-01, -6.1541e-01, -1.9169e-01,\n",
       "        -3.5340e-01,  5.1067e-01, -1.2209e-01,  1.3499e-01, -2.4605e-01,\n",
       "         1.6542e-01,  8.4141e-02,  1.2456e-02, -1.8900e-01, -3.0127e-02,\n",
       "         2.5349e-01, -3.2887e-01, -3.0173e-01,  1.6007e-01, -4.6173e-01,\n",
       "         3.5957e-01,  3.4178e-01, -2.9196e-02,  3.5343e-01, -1.0622e-01,\n",
       "        -2.0769e-01, -3.2061e-03,  5.7141e-01,  3.2355e-03,  8.5519e-02,\n",
       "        -1.3629e-01, -4.2617e-01, -1.8347e-01,  5.3052e-01, -8.6658e-02,\n",
       "        -3.1647e-01,  3.0027e-01,  3.3169e-01, -1.0480e-01,  1.1173e-01,\n",
       "        -1.7976e-01,  1.5568e-01,  4.1122e-01,  3.9273e-03, -3.5037e-01,\n",
       "         3.7225e-02, -4.5402e-01, -5.1914e-02, -3.0151e-01, -1.1680e-03,\n",
       "        -1.7734e-01, -9.7764e-04, -4.1665e-02, -2.5575e-01,  4.5194e-01,\n",
       "         5.5494e-01,  1.0924e-01,  5.3436e-01,  5.2632e-02, -2.9791e-01,\n",
       "        -3.4795e-01,  1.5115e-02, -2.1822e-02,  2.2991e-01,  1.3228e-01,\n",
       "         3.9475e-02, -1.3952e-01,  1.1948e-01,  1.6816e-01,  1.8784e-01,\n",
       "        -3.8676e-01,  5.2806e-02,  7.2230e-02, -6.8298e-02, -1.7035e-03,\n",
       "        -1.6499e-01,  5.1603e-01, -3.4317e-01, -9.5077e-02,  1.7234e-01,\n",
       "        -9.9538e-02,  2.1086e-01,  2.0551e-01, -2.6345e-01, -1.8109e-01,\n",
       "        -1.7711e-01,  2.3078e-01, -4.6065e-01, -4.1576e-01,  7.1540e-01,\n",
       "        -8.3476e-02,  2.0549e-01,  4.4821e-01,  8.3715e-02,  2.8362e-01,\n",
       "         9.2310e-02,  1.2340e-01,  2.7472e-01,  1.4523e-01, -1.9296e-01,\n",
       "         4.9246e-02,  2.2112e-01, -5.6728e-01, -4.0468e+00, -2.8915e-01,\n",
       "         2.3092e-01, -3.0112e-01, -7.3834e-02,  1.0870e-01, -2.7607e-01,\n",
       "        -1.3785e-01, -3.8702e-01, -2.4093e-01, -7.5125e-02, -2.0455e-01,\n",
       "         6.4273e-02,  1.9236e-01, -7.5631e-02,  2.2058e-01,  1.8420e-01,\n",
       "         1.1545e-02, -3.8335e-01,  1.9872e-01, -1.6926e-01,  1.0864e-01,\n",
       "         5.0524e-02, -1.9621e-03,  5.7970e-01, -7.8631e-02, -3.5160e-01,\n",
       "         5.6656e-02, -1.7207e-02, -6.7878e-02, -1.0384e-01, -1.9383e-01,\n",
       "         2.6118e-02,  1.9041e-01,  1.6426e-01,  1.5478e-02,  2.1633e-01,\n",
       "        -1.7825e-01, -7.6367e-02, -2.2156e-01,  2.9030e-01, -2.4995e-01,\n",
       "         1.6510e-01, -9.6134e-02,  8.4345e-01, -7.7006e-01, -3.8122e-01,\n",
       "        -1.2964e-01,  1.0463e-01, -4.1685e-02, -1.6034e-01, -3.2142e-01,\n",
       "        -1.6102e-01, -1.4600e-01, -2.3333e-01, -3.2166e-01,  4.0345e-02,\n",
       "         6.2196e-01, -2.6269e-01, -1.3609e-02,  3.2023e-01, -5.9218e-02,\n",
       "        -4.5166e-02, -2.7928e-01, -2.8348e-01, -1.5139e-01, -7.4738e-01,\n",
       "        -2.6648e-01, -1.9639e-01, -1.8819e-01, -2.7064e-01,  4.6655e-01,\n",
       "        -4.1759e-01, -1.2279e+00,  5.4104e-02, -1.9573e-01, -9.9898e-02,\n",
       "        -1.7358e-01,  1.6602e-01, -1.4329e-01, -4.8575e-01, -6.2515e-02,\n",
       "        -2.1695e-01, -2.7482e-01, -5.2171e-01, -7.1333e-01, -3.7415e-02,\n",
       "        -1.8509e-01, -3.5085e-01, -3.5133e-01,  1.9266e-01,  4.5559e-01,\n",
       "         1.0503e-01,  2.5961e-01,  1.4327e-01,  1.1383e-02,  1.4183e-01,\n",
       "         1.3462e-01,  2.4260e-01,  9.9049e-02,  2.6303e-01, -2.4335e-01,\n",
       "         6.5426e-01, -4.0798e-02, -3.8272e-01,  4.3133e-01, -3.0686e-01,\n",
       "         3.8406e-02,  6.3986e-02,  1.9582e-01, -3.1281e-01, -1.8074e-01,\n",
       "         2.1599e-02, -1.7547e-01, -5.7664e-02,  2.2226e-02,  9.1970e-02,\n",
       "         2.7216e-01, -7.0287e-03, -1.6757e-01, -2.4763e-01,  7.2786e-01,\n",
       "         8.2378e-02, -4.0852e-01,  2.3142e-01,  2.4715e-01,  5.6790e-01,\n",
       "        -3.9762e-02,  2.8969e-01, -4.9804e-01, -5.7898e-02, -3.3115e-01,\n",
       "        -3.3721e-02, -2.8274e-01,  2.9602e-01, -1.3151e-01, -7.3414e-02,\n",
       "        -1.0551e-01,  6.7581e-02,  2.2021e-01,  2.7552e-01,  3.0074e-01,\n",
       "        -2.2903e-02,  8.3781e-02, -3.3120e-02,  4.9769e-01, -2.5518e-02,\n",
       "         4.5313e-01, -2.1258e-01,  2.1076e-01, -5.9558e-01, -3.0283e-01,\n",
       "         1.2728e-01, -3.4864e-01,  2.4339e-01, -3.2979e-01,  3.3657e-01,\n",
       "         2.1204e-01, -4.1882e-01, -4.8920e-01,  1.0351e-01, -1.5341e-01,\n",
       "         1.7295e-01,  3.1890e-02,  2.1855e-01,  4.0052e-01,  3.0048e-02,\n",
       "         2.1909e-01, -3.1269e-01, -2.8154e-02, -1.4879e-02,  2.7744e-01,\n",
       "        -4.7707e-02, -1.7584e-01, -9.1162e-03,  4.6425e-01, -1.1208e-01,\n",
       "        -3.7172e-01, -6.6568e-02,  2.6561e-01, -2.0251e-02,  3.2226e-01,\n",
       "        -2.0509e-01, -1.8487e-02,  3.7265e-01,  1.1065e-01,  8.3084e-02,\n",
       "        -1.4461e-01, -8.5810e-02,  5.1437e-01, -4.2430e-01,  6.3289e-01,\n",
       "         1.2577e-01, -7.0621e-01, -6.3270e-02, -2.0159e-01,  2.5872e-01,\n",
       "        -5.4736e-01,  4.4557e-02,  2.8792e-02, -2.5311e-01,  2.1513e-01,\n",
       "        -3.9876e-01,  1.4195e-01,  1.5288e-01, -1.4742e-01,  1.3059e-02,\n",
       "        -8.8841e-02,  4.8976e-02,  3.2314e-01,  1.4725e-01, -3.0385e-01,\n",
       "        -6.6984e-01, -7.4742e-02, -2.3717e-02, -4.2279e-02, -3.1868e-02,\n",
       "         1.2669e-01, -1.1857e-01,  1.6623e-01, -7.0776e-01,  2.3151e-01,\n",
       "         1.0045e-01,  3.9023e-01, -5.7994e-01, -1.2647e-01,  5.7246e-02,\n",
       "        -2.0107e-03, -3.7781e-01, -2.6828e-01,  5.9997e-03, -3.5208e-01,\n",
       "         1.8688e-01,  1.9671e-01, -2.0347e-01,  4.7706e-02,  1.6002e-01,\n",
       "         7.0317e-02,  2.6237e-01, -1.3885e-01, -2.1373e-01, -2.7020e-01,\n",
       "        -2.3194e-01, -2.1952e-01,  4.4493e-01, -3.8828e-01, -2.9500e-01,\n",
       "         3.5615e-01, -4.0862e-01, -3.0352e-01,  3.5877e-02, -3.6377e-01,\n",
       "        -4.1754e-01, -2.9984e-01, -2.8430e-02, -1.0167e+00, -6.3531e-02,\n",
       "         1.5962e-01, -4.4050e-01, -3.3050e-01, -2.4364e-01,  3.3844e-02,\n",
       "         5.9280e-02,  2.2611e-01, -9.4026e-02, -2.0128e-02,  3.0423e-01,\n",
       "        -1.9152e-01,  1.3604e-01, -3.1933e-01, -1.8747e-01, -1.0597e-01,\n",
       "         1.2370e-01,  2.7467e-01, -1.0392e-01, -2.0732e-01,  3.2512e-02,\n",
       "        -5.1882e-01,  1.3333e-01, -2.4086e-01,  9.4421e-02,  1.8796e-01,\n",
       "        -3.0954e-02, -3.1265e-01,  4.0019e-01, -4.1121e-01, -6.4070e-02,\n",
       "         1.6748e-01,  2.2304e-01,  1.1740e-01, -7.3405e-02,  1.5641e-01,\n",
       "         1.2830e-01, -9.9127e-02,  4.2186e-01,  2.1903e-02,  2.9525e-01,\n",
       "        -1.1872e-01,  1.2821e-01,  4.9335e-02, -2.8621e-02, -1.1619e-01,\n",
       "         4.0285e-01,  1.4492e-01,  2.2057e-01, -8.5684e-02, -3.5027e-01,\n",
       "         4.3343e-02, -4.0036e-02, -1.5064e-01, -5.9688e-01, -3.7171e-01,\n",
       "         3.5427e-01,  4.2581e-01, -5.4735e-01,  3.3330e-01, -8.7164e-02,\n",
       "        -7.1967e-01,  3.0588e-02,  2.8434e-01, -2.0845e-01,  3.9199e-02,\n",
       "         1.2504e-02,  5.2513e-02, -1.6709e-01,  5.1218e-01, -1.3381e-01,\n",
       "        -4.9384e-01,  4.2872e-01,  2.5809e-02, -1.8233e-01,  3.3747e-01,\n",
       "         4.7136e-03,  6.3359e-01, -1.6170e-01, -4.3202e-01, -8.4620e-02,\n",
       "         2.9167e-01,  1.0030e-01,  3.6938e-01,  9.4323e-02, -1.4994e-01,\n",
       "         1.1946e-01,  3.4113e-01, -1.2679e-01,  4.6719e-01,  3.2721e-01,\n",
       "         1.0702e-01,  9.6425e-02,  3.3044e-01,  6.8103e-02,  7.5145e-02,\n",
       "         3.1330e-03, -1.2750e-01,  3.9309e-01,  2.6464e-02,  1.3708e-01,\n",
       "         2.5137e-01,  1.5755e-01,  7.0408e-02,  1.7342e-01, -6.8657e-02,\n",
       "         1.3244e-01, -4.0509e-01,  1.5921e-01,  3.6215e-01,  6.5670e-02,\n",
       "         6.4353e-02, -1.3232e-01, -9.8971e-02, -1.8286e-01,  2.8688e-01,\n",
       "        -6.4956e-02,  2.6784e-02, -3.3641e-01,  7.1636e-01,  5.9654e-02,\n",
       "        -1.2079e-01, -4.6498e-02, -2.5462e-01, -1.3122e-01, -3.5220e-03,\n",
       "        -2.1516e-01,  2.8471e-01,  1.9732e-01, -2.6464e-01,  6.6191e-02,\n",
       "         1.6899e-01,  1.8545e-02, -5.1237e-01,  1.2604e-01, -2.2684e-02,\n",
       "         4.2773e-01, -2.5901e-01, -1.0273e-03, -2.3025e-01,  1.4595e-01,\n",
       "         2.2122e-01,  4.2560e-01,  3.2621e-01,  1.2389e-02, -7.1573e-01,\n",
       "        -3.1195e-01, -1.6622e-01, -1.2132e-01,  2.3915e-02,  1.5820e-01,\n",
       "        -7.0488e-03, -2.4936e-01,  2.0736e-01, -3.2394e-01,  1.0426e-01,\n",
       "        -7.0542e-01,  2.1032e-01, -4.7974e-02,  3.4006e-01,  2.4773e-01,\n",
       "        -1.0416e-01, -3.8555e-01,  9.4925e-02, -3.9937e-01, -4.5851e-02,\n",
       "        -2.0218e-01,  1.3558e-01,  9.7464e-03,  1.8977e-01, -2.4982e-01,\n",
       "         9.6937e-02, -3.5878e-02, -2.1181e-01, -5.3203e-02,  4.6399e-02,\n",
       "         1.1781e-01,  1.4572e-01, -2.2048e-01, -9.8686e-02,  2.6679e-02,\n",
       "         2.7178e-01,  2.5133e-01, -1.0711e-01,  6.4615e-02, -1.6204e-01,\n",
       "        -6.5861e-02, -9.1557e-03, -2.5515e-01, -1.3743e-01,  1.2563e-01,\n",
       "        -1.9512e-01, -1.1227e-02, -4.0575e-01, -1.1866e-01,  1.9595e-01,\n",
       "        -2.2030e-01, -5.9188e-01, -2.0106e-02,  3.1639e-02,  2.9357e-02,\n",
       "        -9.6669e-02,  2.7429e-01,  4.0120e-02])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_embedding[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0d79c67f-63b9-4b87-be70-82c3010ae71b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity Score: 0.90691483\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.90691483]], dtype=float32)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example sentence for similarity comparison\n",
    "example_sentence = \"GeeksforGeeks is a technology website\"\n",
    "\n",
    "# Tokenize and encode the example sentence\n",
    "example_encoding = tokenizer.batch_encode_plus(\n",
    "    [example_sentence],\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors='pt',\n",
    "    add_special_tokens=True\n",
    ")\n",
    "example_input_ids = example_encoding['input_ids']\n",
    "example_attention_mask = example_encoding['attention_mask']\n",
    "\n",
    "# Generate embeddings for the example sentence\n",
    "with torch.no_grad():\n",
    "    example_outputs = model(example_input_ids, attention_mask=example_attention_mask)\n",
    "    example_sentence_embedding = example_outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "# Compute cosine similarity between the original sentence embedding and the example sentence embedding\n",
    "similarity_score = cosine_similarity(sentence_embedding[0].view(1,768), example_sentence_embedding)\n",
    "\n",
    "# Print the similarity score\n",
    "print(\"Cosine Similarity Score:\", similarity_score[0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081be150-3d92-45cc-8c8c-c25893de234b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
